---
title: "Time Series Midterm"
author: "Dongwen Ou"
date: "2024-04-15"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

## Question 1:
### 1.(a)
For the unit root test, we will use ADF Test, for which the relative hypothesis isï¼›
H0: The time series has unit root, so it's non-stationary.
H1: The time series doesn't have unit root, so it's stationary.

```{r}
# Load the necessary library
library(tseries)

# Read the dataset
data_arima <- read.table("C:/Users/Synlim/Desktop/ts_midterm/Dataset-ARIMA.txt", header = FALSE)
colnames(data_arima) <- c("Volatility")
head(data_arima)

# Conduct the Augmented Dickey-Fuller Test
adf_test <- adf.test(data_arima$Volatility, alternative = "stationary")

# Print the results of the ADF test
print(adf_test)

```
As we run the code chunk above, we can get the warning:
"Warning: p-value smaller than printed p-value"
As a result, we can confidently reject the null hypothesis so that the series is stationary.

### 1.(b) We need to decide the order of ARIMA(p,i,q)
We've known that we don't need another difference, so i == 0.
```{r}
# Load additional library for plotting
library(forecast)

# Plotting ACF and PACF
par(mfrow=c(1,2))
Acf(data_arima$Volatility, main="ACF of Volatility")
Pacf(data_arima$Volatility, main="PACF of Volatility")

# Use auto.arima to find the best fit
best_fit <- auto.arima(data_arima$Volatility, d = 0, approximation=FALSE, trace=TRUE)
arima(data_arima$Volatility, order = c(5,0,5))

# Display the best fit
print(best_fit)
summary(best_fit)
```
As we see from the ACF and PACF plots, the cut-off point might be arima(1,0,1) or arima(5,0,5).

Through AIC calculation, we can get the arima(5,0,5) has only a slightly lower AIC compared to arima(1,0,1).

As a result, we use both of the models for the later problems and see which makes more sense.

### 1.(c)
```{r}
fit_arima1 <- arima(data_arima$Volatility, order=c(1,0,1))
summary(fit_arima1)
significance1 <- c(0.9341/0.0568, -0.8547/0.0820, 2.9241/0.2707)
print(significance1)

fit_arima2 <- arima(data_arima$Volatility, order=c(5,0,5))
summary(fit_arima2)
significance2 <- c(0.7409, -0.1767, 0.2555, -0.8896,0.7592, -0.6895, 0.213, -0.2448, 0.9800, -0.6538, 2.8998)/c(0.1790, 0.0631, 0.0544, 0.0752, 0.1496, 0.2053, 0.062, 0.0483, 0.0691, 0.1981, 0.2311)
print(significance2)
```
We may see the results above, and see all the coefficients are statistically significant.

### 1.(d)
We will use Ljung-Box to do hypothesis testing, for which
H0: the residuals follow a white noise.
H1: the residuals don't follow a white noise.
```{r}
# Check residuals
residuals1 <- residuals(fit_arima1)
Acf(residuals1, main="ACF of Residuals")

# Ljung-Box Test
Box.test(residuals1, type="Ljung-Box")

residuals2 <- residuals(fit_arima2)
Acf(residuals2, main="ACF of Residuals")

# Ljung-Box Test
Box.test(residuals2, type="Ljung-Box")

```
For both of the case, p-value > 0.05, then we fail to reject the null hypothesis, which indicates that the residuals follow a white noise.
But we can see the arima(5,0,5) seems to have a more white noise residual.

### 1.(e) k-step ahead forecasting
```{r}
# Forecasting
forecast_arima1 <- forecast(fit_arima1, h=5)
print(forecast_arima1)
plot(forecast_arima1)

forecast_arima2 <- forecast(fit_arima2, h=5)
print(forecast_arima2)
plot(forecast_arima2)
```
The two tables in the output have shown the respective forecast value and the lower&upper bound of the confidence interval at significance level 80% and 95%.

The two models have totally different forecasting results, but from my perspective, I prefer the arima(5,0,5) model as the results from arima(1,0,1) are always near 3 when we increase the step k. Therefore, arima(5,0,5) may catch more volatility and information.
----------------------------------------------------------------------------------------------------

## Question 2:
### 2.(a)
We'll use Lagrange multiplier test to check ARCH effect, for which
H0: Coefficients are all equal to 0, namely, no ARCH effect
H1: There exists at least one coeffcient != 0, namely, there is ARCH effect 
```{r}
data_garch <- read.table("C:/Users/Synlim/Desktop/ts_midterm/Dataset-GARCH.txt", header=TRUE)
data_garch$log_returns <- log(data_garch$rtn + 1) 
head(data_garch)
#print(data_garch$log_returns)
#install.packages('FinTS')
library(FinTS)
# Perform the Lagrange Multiplier test for ARCH effects
ArchTest_result <- ArchTest(data_garch$log_returns, lag = 12) 
print(ArchTest_result)
```
We finally got a p-value = 1.855e-05 < 0.05, so we can confidently reject H0 admitting that there is ARCH effect in the series.

### 2.(b)
We first check the acf and pacf to help determine the order of the mean equation, and we directly set order of GARCH as (1,1). Because we can see from the code chunk after the below one, during the for loop, the AIC are almost the same, but the coefficients larger than the first order may not be statistically significant anymore.

```{r}
acf(data_garch$log_returns)
pacf(data_garch$log_returns)
library(rugarch)

spec1 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
                   mean.model = list(armaOrder = c(0, 0), include.mean = TRUE))

# Fit the model
fit1 <- ugarchfit(spec = spec1, data = data_garch$log_returns)
fit1

spec2 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
                   mean.model = list(armaOrder = c(7, 7), include.mean = TRUE))

# Fit the model
fit2 <- ugarchfit(spec = spec2, data = data_garch$log_returns)
fit2
```
From the above results, we've difined two GARCH models, we only write down the fit1 model here, which is:
$r_t = 0.01633 + a_t$
$\sigma_t^2 = 0.001091 + 0.079785a_{t-1}^2 + 0.85546\sigma_{t-1}^2$


```{r}
for (i in 1:3) {
  for (j in 1:3){
    spec1 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(i, j)),
                   mean.model = list(armaOrder = c(0, 0), include.mean = TRUE))

    # Fit the model
    fit1 <- ugarchfit(spec = spec1, data = data_garch$log_returns)
    print(fit1)
  }
}
```



### 2.(c)
```{r}

# Assuming fit1 is your fitted GARCH model
res1 <- residuals(fit1, standardize=TRUE)

# Conduct a Ljung-Box test
Box.test(res1, lag=10, type="Ljung-Box")

# Check the distribution of standardized residuals
hist(res1, breaks=50, main="Histogram of Standardized Residuals")
qqnorm(res1)
qqline(res1)

# ARCH LM test on squared standardized residuals
ArchTest(res1^2)
#--------------------------------------------------------------
# Assuming fit1 is your fitted GARCH model
res2 <- residuals(fit2, standardize=TRUE)

# Conduct a Ljung-Box test
Box.test(res2, lag=10, type="Ljung-Box")

# Check the distribution of standardized residuals
hist(res2, breaks=50, main="Histogram of Standardized Residuals")
qqnorm(res2)
qqline(res2)

# ARCH LM test on squared standardized residuals
ArchTest(res2^2)
```

From the outcomes above, we have several conclusions:
For Box-Ljung test, the p-value of which is 0.4571 > 0.05, we fail to reject the null hypothesis of no autocorrelation. -> A white noise residual!

Then, perform the ARCH-LM (Lagrange Multiplier) test on the squared residuals to check for any remaining ARCH effects. The p-value is 0.9996 > 0.05, so we fail to reject H0 which is no remaining ARCH effect.
(But under 0.05, mean model(7,7) can be rejected unless under 0.01)

From the two plots above, we can see it to some extent follows a normal distribution.

### 2.(d)
```{r}
library(forecast)
forecast_vol1 <- ugarchforecast(fit1, n.ahead = 5)
forecast_vol1@forecast$sigmaFor

forecast_vol2 <- ugarchforecast(fit2, n.ahead = 5)
forecast_vol2@forecast$sigmaFor
```


### 2.(e)
```{r}
forecast_mean <- ugarchforecast(fit1, n.ahead = 5)
forecast_mean@forecast$seriesFor

forecast_mean2 <- ugarchforecast(fit2, n.ahead = 5)
forecast_mean2@forecast$seriesFor
```


